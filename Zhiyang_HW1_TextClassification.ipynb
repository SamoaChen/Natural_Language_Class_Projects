{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Zhiyang_HW1_TextClassification.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamoaChen/Natural_Language_Class_Projects/blob/main/Zhiyang_HW1_TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "published-rating"
      },
      "source": [
        "#-----IMPORT PACKAGES\n",
        "import numpy as np\n",
        "import math\n",
        "import nltk \n",
        "import random\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import PorterStemmer\n",
        "lancaster=LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "ps = PorterStemmer()"
      ],
      "id": "published-rating",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "large-province"
      },
      "source": [
        "#-----TRAINING AND TESTING FUNCTIONS\n",
        "#CREATE A CLASS FOR MODEL PARAMETERS\n",
        "class TrainParameters:\n",
        "    def __init__(self,logprior, loglikelihood, classes, V, weight):\n",
        "        self.logprior = logprior\n",
        "        self.loglikelihood = loglikelihood\n",
        "        self.classes = classes\n",
        "        self.weight = weight\n",
        "        self.V = V\n",
        "        \n",
        "#CREATE THE TRAINING FUNCTION\n",
        "def naive_bayes_train(file_name, stop_word_num, smoothe, coef):\n",
        "    class_dict = dict()\n",
        "\n",
        "    #INITIATE LOCATION FOR STORING DOC NUMBER\n",
        "    class_dict['N_doc'] = 0 \n",
        "    \n",
        "    #INITIATE LOCATION FOR STORING ALL WORDS\n",
        "    V = dict()\n",
        "    \n",
        "    #CALCULATE STOP WORDS\n",
        "    stop_words = find_stop_words(file_name, stop_word_num)\n",
        "\n",
        "    #READ FILE\n",
        "    train_file = open(file_name,'r')\n",
        "\n",
        "    #ITERATE EVERY DOCUMENT\n",
        "    for line in train_file:\n",
        "        train_doc_info = line.split()\n",
        "        #POPUATE CLASS_DICT DICTIONARIES\n",
        "        if train_doc_info[1] in class_dict.keys():\n",
        "            class_dict[train_doc_info[1]]['N_class'] += 1\n",
        "        else:\n",
        "            class_dict[train_doc_info[1]] = dict()\n",
        "            class_dict[train_doc_info[1]]['N_class'] = 1\n",
        "            class_dict[train_doc_info[1]]['N_word'] =0\n",
        "        #COUNT DOCUMENT NUMBER\n",
        "        class_dict['N_doc'] += 1\n",
        "        \n",
        "        #LOOP WORDS FROM EACH CLASS\n",
        "        text = open(train_doc_info[0]).read()\n",
        "        text_tokenized = tknzr.tokenize(text)\n",
        "        #REPLACE WORD WITH POS (doesn't work)\n",
        "        #text_tokenized = [t for w, t in nltk.pos_tag(text_tokenized)]\n",
        "        \n",
        "        for word in text_tokenized:\n",
        "            #word = lancaster.stem(word) #doesn't workd\n",
        "            #word = lemmatizer.lemmatize(word) #(doesn't work)\n",
        "            #word = ps.stem(word) #(doesn't work)\n",
        "            #if word in string.punctuation:   #doesn't work\n",
        "                #continue\n",
        "            if word in stop_words:\n",
        "                continue\n",
        "            elif word not in class_dict[train_doc_info[1]].keys():\n",
        "                class_dict[train_doc_info[1]][word] = 1\n",
        "                class_dict[train_doc_info[1]]['N_word'] += 1\n",
        "            else:\n",
        "                class_dict[train_doc_info[1]][word] += 1\n",
        "                class_dict[train_doc_info[1]]['N_word'] += 1\n",
        "            #TABULATE WORD TO V DICTIONARY\n",
        "            if word not in V.keys():\n",
        "                V[word] = 1\n",
        "            else:\n",
        "                V[word] += 1\n",
        "    \n",
        "    #CLOSE TRAIN_FILE OBJECT\n",
        "    train_file.close()\n",
        "    \n",
        "    #CREATE LIST OF CLASSES\n",
        "    classes = list()\n",
        "    for i in class_dict.keys():\n",
        "        classes.append(i)\n",
        "    classes.remove('N_doc')\n",
        "    \n",
        "    #SELECT THE SMOOTHING METHOD\n",
        "    if smoothe == 'lap':\n",
        "        return loglikelihood_laplace(class_dict, V, classes, coef)\n",
        "    elif smoothe == 'JM':\n",
        "        return loglikelihood_jelinek_mercer(class_dict, V, classes, coef)\n",
        "    elif smoothe == 'dir':\n",
        "        return loglikelihood_dirichlet(class_dict, V, classes, coef)\n",
        "\n",
        "    \n",
        "#CREATE TRAINING FUNCTION FOR THE BAGGING APPROACH\n",
        "def naive_bayes_bagging_train(file_name, bags, portion_percent): \n",
        "    #INITIATE A LIST TO STORE ALL TRAINED MODEL OBJECT\n",
        "    bag_list = []\n",
        "    #CREATE A NUMBER ARRAY TO SELECT FROM\n",
        "    line_count = sum(1 for line in open(file_name))\n",
        "    number_list = np.arange(1, line_count+1,1)\n",
        "    number_in_bag = int(line_count*portion_percent)\n",
        "    #TRAINING FOR EACH BAG\n",
        "    for bag in range(bags):\n",
        "        partition_list = np.random.choice(number_list, number_in_bag, replace = True)\n",
        "        write_train_bag(partition_list, file_name)\n",
        "        logprior, loglikelihood, classes, V = naive_bayes_train('bag.labels', 25, 'lap', 0.094)\n",
        "        naive_bayes_test_validation('test.labels', logprior, loglikelihood, classes, V)\n",
        "        right, wrong, weight = compute_accuracy('test.labels','predicted_for_bagging.labels')\n",
        "        bag_list.append(TrainParameters(logprior, loglikelihood, classes, V, weight*(right+wrong)))\n",
        "    \n",
        "    return bag_list\n",
        "\n",
        "#CREATE TESTING FUNCTION FOR THE BAGGING APPROACH\n",
        "def naive_bayes_bagging_test(test_file_name, bag_list):\n",
        "    #CREATE OUTPUT FILE\n",
        "    output_file = open('predicted.labels', 'w+')\n",
        "    \n",
        "    #READ TEST FILE\n",
        "    test_file = open(test_file_name, 'r')\n",
        "    \n",
        "    #ITERATE THROUGH EVERY DOCUMENT\n",
        "    for line in test_file:\n",
        "        #INITIATE DICTIONARY TO STORE CLASS COUNT GIVEN BY EACH MODEL\n",
        "        class_count_bagging = dict()\n",
        "        test_doc_info = line.split()\n",
        "        \n",
        "        #TOKENIZE WORDS\n",
        "        text = open(test_doc_info[0]).read()\n",
        "        text_tokenized = tknzr.tokenize(text)\n",
        "        \n",
        "        #ITERATE THROUGH ALL MODELS\n",
        "        for bag in range(len(bag_list)):\n",
        "            #INITIATE DICTIONARY TO STORE CLASS PROBABILITIES\n",
        "            class_prob = dict()\n",
        "            \n",
        "            for class_type in bag_list[bag].classes:\n",
        "                class_prob[class_type] = bag_list[bag].logprior[class_type]\n",
        "                for word in text_tokenized:\n",
        "                    if word in bag_list[bag].V.keys():\n",
        "                        class_prob[class_type] += bag_list[bag].loglikelihood[class_type][word]\n",
        "            \n",
        "            max_class = sorted(class_prob, key=class_prob.get, reverse=True)[0]\n",
        "            \n",
        "            if max_class not in class_count_bagging.keys():\n",
        "                class_count_bagging[max_class] = 1*bag_list[bag].weight\n",
        "            else:\n",
        "                class_count_bagging[max_class] += 1*bag_list[bag].weight\n",
        "        \n",
        "        #FIND THE CLASS THAT IS THE MOST CLASSIFIED\n",
        "        max_class_bagging = sorted(class_count_bagging, key=class_count_bagging.get, reverse=True)[0]\n",
        "        \n",
        "        #WRITE TO DOCUMENT\n",
        "        output_file.write(test_doc_info[0]+' '+max_class+'\\n')\n",
        "    \n",
        "    output_file.close()\n",
        "    test_file.close()\n",
        "\n",
        "#CREATE TESTING FUNCTION\n",
        "def naive_bayes_test_validation(test_file_name, logprior, loglikelihood, classes, V):  \n",
        "    #CREATE OUTPUT FILE\n",
        "    output_file = open('predicted_for_bagging.labels', 'w+')\n",
        "    \n",
        "    #READ TEST FILE\n",
        "    test_file = open(test_file_name, 'r')\n",
        "    \n",
        "    #ITERATE EVERY DOCUMENT\n",
        "    for line in test_file:\n",
        "        #INITIATE DICTIONARY TO STORE CLASS PROBABILITIES\n",
        "        class_prob = dict()\n",
        "        test_doc_info = line.split()  \n",
        "        \n",
        "        #TOKENIZE WORDS\n",
        "        text = open(test_doc_info[0]).read()\n",
        "        text_tokenized = tknzr.tokenize(text)\n",
        "        \n",
        "        for class_type in classes:\n",
        "            class_prob[class_type] = logprior[class_type]\n",
        "            for word in text_tokenized:\n",
        "                if word in V.keys():\n",
        "                    class_prob[class_type] += loglikelihood[class_type][word]\n",
        "        \n",
        "        #FIND THE CLASS THAT OUTPUTS LARGEST PROBABILITY\n",
        "        max_class = sorted(class_prob, key=class_prob.get, reverse=True)[0]\n",
        "        \n",
        "        #WRITE TO DOCUMENT\n",
        "        output_file.write(test_doc_info[0]+' '+max_class+'\\n')\n",
        "    \n",
        "    output_file.close()\n",
        "    test_file.close()\n",
        "    \n",
        "#CREATE TESTING FUNCTION\n",
        "def naive_bayes_test(test_file_name, logprior, loglikelihood, classes, V):  \n",
        "    #CREATE OUTPUT FILE\n",
        "    output_file = open('predicted.labels', 'w+')\n",
        "    \n",
        "    #READ TEST FILE\n",
        "    test_file = open(test_file_name, 'r')\n",
        "    \n",
        "    #ITERATE EVERY DOCUMENT\n",
        "    for line in test_file:\n",
        "        #INITIATE DICTIONARY TO STORE CLASS PROBABILITIES\n",
        "        class_prob = dict()\n",
        "        test_doc_info = line.split()  \n",
        "        \n",
        "        #TOKENIZE WORDS\n",
        "        text = open(test_doc_info[0]).read()\n",
        "        text_tokenized = tknzr.tokenize(text)\n",
        "        \n",
        "        for class_type in classes:\n",
        "            class_prob[class_type] = logprior[class_type]\n",
        "            for word in text_tokenized:\n",
        "                if word in V.keys():\n",
        "                    class_prob[class_type] += loglikelihood[class_type][word]\n",
        "        \n",
        "        #FIND THE CLASS THAT OUTPUTS LARGEST PROBABILITY\n",
        "        max_class = sorted(class_prob, key=class_prob.get, reverse=True)[0]\n",
        "        \n",
        "        #WRITE TO DOCUMENT\n",
        "        output_file.write(test_doc_info[0]+' '+max_class+'\\n')\n",
        "    \n",
        "    output_file.close()\n",
        "    test_file.close()\n",
        "\n",
        "\n",
        "#CREATE FUNCTION FOR FINDING STOP WORDS\n",
        "def find_stop_words(file_name, stop_word_num):\n",
        "    #INITIATE LOCATION FOR STORING ALL WORDS\n",
        "    V = dict()\n",
        "    \n",
        "    #READ FILE\n",
        "    train_file = open(file_name,'r') \n",
        "    \n",
        "    #ITERATE EVERY DOCUMENT\n",
        "    for line in train_file:    \n",
        "        train_doc_info = line.split()\n",
        "        \n",
        "        #LOOP WORDS FROM EACH CLASS\n",
        "        text = open(train_doc_info[0]).read()\n",
        "        text_tokenized = tknzr.tokenize(text)\n",
        "        \n",
        "        for word in text_tokenized:        \n",
        "            if word in string.punctuation:\n",
        "                continue        \n",
        "        \n",
        "            #TABULATE WORD TO V DICTIONARY\n",
        "            if word not in V.keys():\n",
        "                V[word] = 1\n",
        "            else:\n",
        "                V[word] += 1\n",
        "    \n",
        "    #CLOSE TRAIN_FILE OBJECT\n",
        "    train_file.close()\n",
        "    \n",
        "    #FIND 10 STOP WORDS \n",
        "    stop_words = sorted(V, key=V.get, reverse=True)[:stop_word_num]        \n",
        "    \n",
        "    return stop_words\n",
        "\n",
        "\n",
        "        \n",
        "#CREATE FUNCTION FOR LAPACE SMOOTHING LOG LIKELIHOOD\n",
        "def loglikelihood_laplace(class_dict, V, classes, coef):\n",
        "    #SMOOTHING FACTOR\n",
        "    lap = coef\n",
        "    #CREATE LOGPRIOR AND LOGLIKELIHOOD DICTIONARIES\n",
        "    logprior = dict()\n",
        "    loglikelihood = dict()\n",
        "    #CALCULATE MAGNITUDE OF V\n",
        "    V_length = len(list(V))\n",
        "    \n",
        "    #CALCULATE THE LOGPRIOR\n",
        "    for class_type in classes:\n",
        "        logprior[class_type] = math.log(class_dict[class_type]['N_class']/class_dict['N_doc'])\n",
        "    \n",
        "    #CALCULATE THE LOGLIKELIHOOD\n",
        "    for word in V:\n",
        "        for class_type in classes:\n",
        "            if word in class_dict[class_type].keys():\n",
        "                word_loglikelihood = math.log((class_dict[class_type][word]+lap)/(class_dict[class_type]['N_word']+V_length*lap))\n",
        "            else:\n",
        "                word_loglikelihood = math.log(lap/(class_dict[class_type]['N_word']+V_length*lap))\n",
        "            \n",
        "            if class_type not in loglikelihood.keys():\n",
        "                loglikelihood[class_type] = dict()\n",
        "                loglikelihood[class_type][word] = word_loglikelihood\n",
        "            else:\n",
        "                loglikelihood[class_type][word] = word_loglikelihood\n",
        "                \n",
        "\n",
        "    return logprior, loglikelihood, classes, V\n",
        "\n",
        "#CREATE FUNCTION FOR JELINEK-MERCER SMOOTHING LOG LIKELIHOOD\n",
        "def loglikelihood_jelinek_mercer(class_dict, V, classes, coef):\n",
        "    #COEFFICIENT OF INTERPOLATION\n",
        "    beta = coef\n",
        "    \n",
        "    #CREATE LOGPRIOR AND LOGLIKELIHOOD DICTIONARIES\n",
        "    logprior = dict()\n",
        "    loglikelihood = dict()\n",
        "    #CALCULATE MAGNITUDE OF V\n",
        "    V_length = len(list(V))\n",
        "    \n",
        "    #CALCULATE THE LOGPRIOR\n",
        "    for class_type in classes:\n",
        "        logprior[class_type] = math.log(class_dict[class_type]['N_class']/class_dict['N_doc'])\n",
        "    \n",
        "    #CALCULATE THE LOGLIKELIHOOD\n",
        "    for word in V:\n",
        "        #INITIATE PLACE TO STORE NUMERATOR AND DENOMINATOR\n",
        "        numerator = 0\n",
        "        denominator = 0\n",
        "        #BUILDING THE COLLECTION LM, P_w_c\n",
        "        for class_type in classes:\n",
        "            if word in class_dict[class_type].keys():\n",
        "                numerator += class_dict[class_type][word]\n",
        "            denominator += class_dict[class_type]['N_word']\n",
        "        P_w_c = numerator/denominator\n",
        "        \n",
        "        #CALCULATE LOGLIKELIHOOD FOR EACH WORD\n",
        "        for class_type in classes:\n",
        "            if word in class_dict[class_type].keys():\n",
        "                word_loglikelihood = math.log((1-beta)*(class_dict[class_type][word]/class_dict[class_type]['N_word'])+beta*P_w_c)\n",
        "            else:\n",
        "                word_loglikelihood = math.log(beta*P_w_c)\n",
        "                \n",
        "            if class_type not in loglikelihood.keys():\n",
        "                loglikelihood[class_type] = dict()\n",
        "                loglikelihood[class_type][word] = word_loglikelihood\n",
        "            else:\n",
        "                loglikelihood[class_type][word] = word_loglikelihood \n",
        "    \n",
        "    return logprior, loglikelihood, classes, V\n",
        "    \n",
        "#CREATE FUNCTION FOR DIRICHLET PRIOR SMOOTHING LOG LIKELIHOOD\n",
        "def loglikelihood_dirichlet(class_dict, V, classes, coef):\n",
        "    #COEFFICIENT OF INTERPOLATION\n",
        "    mu = coef\n",
        "\n",
        "    #CREATE LOGPRIOR AND LOGLIKELIHOOD DICTIONARIES\n",
        "    logprior = dict()\n",
        "    loglikelihood = dict()\n",
        "    #CALCULATE MAGNITUDE OF V\n",
        "    V_length = len(list(V))\n",
        "    \n",
        "    #CALCULATE THE LOGPRIOR\n",
        "    for class_type in classes:\n",
        "        logprior[class_type] = math.log(class_dict[class_type]['N_class']/class_dict['N_doc'])\n",
        "        \n",
        "    #CALCULATE THE LOGLIKELIHOOD\n",
        "    for word in V:\n",
        "        #INITIATE PLACE TO STORE NUMERATOR AND DENOMINATOR\n",
        "        numerator = 0\n",
        "        denominator = 0\n",
        "        #BUILDING THE COLLECTION LM, P_w_c\n",
        "        for class_type in classes:\n",
        "            if word in class_dict[class_type].keys():\n",
        "                numerator += class_dict[class_type][word]\n",
        "            denominator += class_dict[class_type]['N_word']\n",
        "        P_w_c = numerator/denominator\n",
        "        \n",
        "        #CALCULATE LOGLIKELIHOOD FOR EACH WORD\n",
        "        for class_type in classes:\n",
        "            if word in class_dict[class_type].keys():\n",
        "                word_loglikelihood = math.log((class_dict[class_type]['N_word']/(class_dict[class_type]['N_word']+mu))*(class_dict[class_type][word]/class_dict[class_type]['N_word'])+(mu/(mu+class_dict[class_type]['N_word']))*P_w_c)\n",
        "                #word_loglikelihood = math.log((class_dict[class_type]['N_word']+mu*P_w_c)/(class_dict[class_type]['N_word']+mu))\n",
        "            else:\n",
        "                word_loglikelihood = math.log((mu/(mu+class_dict[class_type]['N_word']))*P_w_c)\n",
        "                #word_loglikelihood = math.log(mu*P_w_c/(class_dict[class_type]['N_word']+mu))\n",
        "\n",
        "                \n",
        "            if class_type not in loglikelihood.keys():\n",
        "                loglikelihood[class_type] = dict()\n",
        "                loglikelihood[class_type][word] = word_loglikelihood\n",
        "            else:\n",
        "                loglikelihood[class_type][word] = word_loglikelihood \n",
        "    \n",
        "    return logprior, loglikelihood, classes, V\n",
        "  \n",
        "#COMPUTE ACCURACY FUNCTION\n",
        "def compute_accuracy(actual_label_file,pred_label_file):\n",
        "    #INITIATE CORRECT AND INCORRECT VARIABLE\n",
        "    correct = 0\n",
        "    incorrect = 0\n",
        "    #COMPARING TWO FILES\n",
        "    actual_label = open(actual_label_file,'r')\n",
        "    pred_label = open(pred_label_file,'r')\n",
        "    for line_actual in actual_label:\n",
        "        line_actual_split = line_actual.split()\n",
        "        #READ ONE LINE FROM PREDICTED FILE\n",
        "        line_pred = pred_label.readline()\n",
        "        line_pred_split = line_pred.split()\n",
        "        #COMPARING\n",
        "        if line_pred_split[1] == line_actual_split[1]:\n",
        "            correct+=1\n",
        "        else:\n",
        "            incorrect+=1\n",
        "    #CLOSING FILES\n",
        "    actual_label.close()\n",
        "    pred_label.close()\n",
        "    #CALCULATE ACCURACY\n",
        "    accuracy = correct/(correct+incorrect)\n",
        "    \n",
        "    return correct, incorrect, accuracy\n",
        "\n",
        "#WRITE TRAIN FILE FOR THE BAGGING APPROACH\n",
        "def write_train_bag(partition_list, file_name):\n",
        "    line_current = 1\n",
        "    bag = open('bag.labels','w+')\n",
        "    test = open('test.labels','w+')\n",
        "    file = open(file_name,'r')\n",
        "    for line in file:\n",
        "        if line_current in partition_list:\n",
        "            iteration = np.sum(partition_list == line_current)\n",
        "            for ite in range(iteration):\n",
        "                bag.write(line)\n",
        "        else:\n",
        "            test.write(line)\n",
        "        line_current += 1\n",
        "    file.close()\n",
        "    bag.close()\n",
        "    test.close()\n",
        "\n",
        "#WRITE TRAIN AND TEST FILES\n",
        "def write_train_test(test_array, file_name):\n",
        "    line_current = 1\n",
        "    train = open('k_fold_folder/train.labels','w+')\n",
        "    test = open('k_fold_folder/test.labels','w+')\n",
        "    file = open(file_name,'r')\n",
        "    #ITERATE AND WRITE THE TRAIN AND TEST FILE\n",
        "    for line in file:\n",
        "        if line_current in test_array:\n",
        "            test.write(line)\n",
        "            line_current += 1\n",
        "        else:\n",
        "            train.write(line)\n",
        "            line_current += 1\n",
        "    file.close()\n",
        "    test.close()\n",
        "    train.close()\n",
        "         \n",
        "    \n",
        "#K FOLD CROSS VALIDATION\n",
        "def k_fold_cross_validation(file_name, K, stop_word_num, smoothe, coef):\n",
        "    #INITATE VARIABLE FOR STORING COREECT AND INCORRECT NUMBER\n",
        "    correct_sum = 0\n",
        "    incorrect_sum = 0\n",
        "    #COUNT LINE NUMBER IN THE FILE\n",
        "    line_count = sum(1 for line in open(file_name))\n",
        "    number_list = np.arange(1,line_count+1,1)\n",
        "    random.shuffle(number_list)\n",
        "    #PARTITION SET INTO K PARTS\n",
        "    partition_list = np.array_split(number_list, K)\n",
        "    #ITERATE K POSSIBLE TRAINS AND TESTS\n",
        "    for list in range(K):\n",
        "        write_train_test(partition_list[list], file_name)\n",
        "        logprior, loglikelihood, classes, V = naive_bayes_train('k_fold_folder/train.labels', stop_word_num, smoothe, coef) \n",
        "        naive_bayes_test('k_fold_folder/test.labels', logprior, loglikelihood, classes, V)\n",
        "        correct, incorrect, _= compute_accuracy('k_fold_folder/test.labels','predicted.labels')\n",
        "        correct_sum += correct\n",
        "        incorrect_sum += incorrect\n",
        "    accuracy = correct_sum/(correct_sum+incorrect_sum)\n",
        "    return accuracy\n",
        "\n",
        "    \n",
        "#logprior, loglikelihood, classes, V = naive_bayes_train(\"train.labels\") \n",
        "#naive_bayes_test('test.labels', logprior, loglikelihood, classes, V)\n",
        "\n",
        "#k_fold_cross_validation('corpus2_train.labels',3,50,'lap', 0.001)"
      ],
      "id": "large-province",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geological-interface"
      },
      "source": [
        "### Sample function call for bagging with naive bayes"
      ],
      "id": "geological-interface"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "civilian-darwin"
      },
      "source": [
        "bag_list = naive_bayes_bagging_train('corpus1_train.labels', 10, 0.6)\n",
        "naive_bayes_bagging_test('corpus1_test.list', bag_list)"
      ],
      "id": "civilian-darwin",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dedicated-repair"
      },
      "source": [
        "### Grid search to find the best parameters for bagging with naive bayes"
      ],
      "id": "dedicated-repair"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paperback-algeria",
        "outputId": "a948454d-a753-4499-e674-0f4440375ffe"
      },
      "source": [
        "#-----LOOP TO FIND THE BEST PARAMETER SETS\n",
        "#INITIATE VALUES TO LOOP THROUGH\n",
        "bag_number_array = np.arange(2,20,1)\n",
        "portion_percent_array = np.arange(0.6, 1.3, 0.1)\n",
        "\n",
        "#INITIATE VARIABLES TO STORE BEST PARAMETERS\n",
        "bag_number_best = 0\n",
        "portion_percent_best = 0\n",
        "accuracy = 0\n",
        "\n",
        "for bag_number in bag_number_array:\n",
        "    for portion_percent in portion_percent_array:\n",
        "        bag_list = naive_bayes_bagging_train('corpus1_train.labels', bag_number, portion_percent)\n",
        "        naive_bayes_bagging_test('corpus1_test.list', bag_list)\n",
        "        _, _, accuracy_new = compute_accuracy('corpus1_test.labels','predicted.labels')\n",
        "        if accuracy_new > accuracy:\n",
        "            accuracy = accuracy_new\n",
        "            bag_number_best = bag_number\n",
        "            portion_percent_best = portion_percent\n",
        "print(\"The best parameters for laplace smoothing bagging are bag_number:%d  portion_percent:%1.2f\" %(bag_number_best,portion_percent_best))"
      ],
      "id": "paperback-algeria",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters for laplace smoothing bagging are bag_number:2  portion_percent:1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brief-secretariat"
      },
      "source": [
        "# Finding the best parameters for three different smoothing methods"
      ],
      "id": "brief-secretariat"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "motivated-chinese",
        "outputId": "d7faccdb-c9e2-4df0-b927-738b4f2e0d90"
      },
      "source": [
        "#-----LOOP TO FIND THE BEST PARAMETER SETS\n",
        "#INITIATE VALUES TO LOOP THROUGH\n",
        "stop_word_array = np.arange(0,30,1)\n",
        "coef_array = np.logspace(-5,0,30, endpoint=True)\n",
        "\n",
        "#INITIATE VARIABLES TO STORE BEST PARAMETERS\n",
        "coef_best = 0\n",
        "stop_word_best = 0\n",
        "accuracy = 0\n",
        "\n",
        "for stop_word in stop_word_array:\n",
        "    for coef in coef_array:\n",
        "        accuracy_new = k_fold_cross_validation('corpus2_train.labels',5,stop_word,'lap', coef)\n",
        "        if accuracy_new > accuracy:\n",
        "            accuracy = accuracy_new\n",
        "            coef_best = coef\n",
        "            stop_word_best = stop_word\n",
        "print(\"The best parameters for laplace smoothing are stop_word:%d  coef:%1.5f\" %(stop_word_best,coef_best))"
      ],
      "id": "motivated-chinese",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters for laplace smoothing are stop_word:28  coef:0.09237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "planned-league",
        "outputId": "16f9dafd-0416-414d-82f3-8c3cfa7cf732"
      },
      "source": [
        "#-----LOOP TO FIND THE BEST PARAMETER SETS\n",
        "#INITIATE VALUES TO LOOP THROUGH\n",
        "stop_word_array = np.arange(24,27,1)\n",
        "coef_array = np.arange(0.09,0.1,0.001)\n",
        "\n",
        "#INITIATE VARIABLES TO STORE BEST PARAMETERS\n",
        "coef_best = 0\n",
        "stop_word_best = 0\n",
        "accuracy = 0\n",
        "\n",
        "for stop_word in stop_word_array:\n",
        "    for coef in coef_array:\n",
        "        accuracy_new = k_fold_cross_validation('corpus3_train.labels',5,stop_word,'lap', coef)\n",
        "        if accuracy_new > accuracy:\n",
        "            accuracy = accuracy_new\n",
        "            coef_best = coef\n",
        "            stop_word_best = stop_word\n",
        "print(\"The best parameters for laplace smoothing are stop_word:%d  coef:%1.5f\" %(stop_word_best,coef_best))"
      ],
      "id": "planned-league",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters for laplace smoothing are stop_word:25  coef:0.09400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "retired-survivor",
        "outputId": "2c483d0c-41d5-4b10-dca9-09dfeea9e318"
      },
      "source": [
        "#PRINT ACCURACY WITH THE ABOVE PARAMETERS FOR THREE CORPUS\n",
        "accuracy_corpus1 = k_fold_cross_validation('corpus1_train.labels',10,28,'lap', 0.09237)\n",
        "accuracy_corpus2 = k_fold_cross_validation('corpus2_train.labels',10,28,'lap', 0.09237)\n",
        "accuracy_corpus3 = k_fold_cross_validation('corpus3_train.labels',10,28,'lap', 0.09237)\n",
        "print(\"Accuracy for corpus 1, 2, 3 are %1.5f%%, %1.5f%%, %1.5f%% respectively\" %(accuracy_corpus1*100, accuracy_corpus2*100, accuracy_corpus3*100))"
      ],
      "id": "retired-survivor",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for corpus 1, 2, 3 are 86.32768%, 83.89262%, 91.51832% respectively\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dramatic-offense",
        "outputId": "83b6d773-5f23-42a6-e66a-215244de4e9d"
      },
      "source": [
        "#-----LOOP TO FIND THE BEST PARAMETER SETS\n",
        "#INITIATE VALUES TO LOOP THROUGH\n",
        "stop_word_array = np.arange(10,30,1)\n",
        "coef_array = np.logspace(-5,0,30, endpoint=True)\n",
        "\n",
        "#INITIATE VARIABLES TO STORE BEST PARAMETERS\n",
        "coef_best = 0\n",
        "stop_word_best = 0\n",
        "accuracy = 0\n",
        "\n",
        "for stop_word in stop_word_array:\n",
        "    for coef in coef_array:\n",
        "        accuracy_new = k_fold_cross_validation('corpus2_train.labels',5,stop_word,'JM', coef)\n",
        "        if accuracy_new > accuracy:\n",
        "            accuracy = accuracy_new\n",
        "            coef_best = coef\n",
        "            stop_word_best = stop_word\n",
        "print(\"The best parameters for JM smoothing are stop_word:%d  coef:%1.5f\" %(stop_word_best,coef_best))"
      ],
      "id": "dramatic-offense",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters for JM smoothing are stop_word:26  coef:0.02807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advisory-effect",
        "outputId": "14e0a691-e48c-4ffb-b708-db003d9f896a"
      },
      "source": [
        "#PRINT ACCURACY WITH THE ABOVE PARAMETERS FOR THREE CORPUS\n",
        "accuracy_corpus1 = k_fold_cross_validation('corpus1_train.labels',10,26,'JM', 0.02807)\n",
        "accuracy_corpus2 = k_fold_cross_validation('corpus2_train.labels',10,26,'JM', 0.02807)\n",
        "accuracy_corpus3 = k_fold_cross_validation('corpus3_train.labels',10,26,'JM', 0.02807)\n",
        "print(\"Accuracy for corpus 1, 2, 3 are %1.5f%%, %1.5f%%, %1.5f%% respectively\" %(accuracy_corpus1*100, accuracy_corpus2*100, accuracy_corpus3*100))"
      ],
      "id": "advisory-effect",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for corpus 1, 2, 3 are 86.66667%, 83.44519%, 90.15707% respectively\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sensitive-surrey",
        "outputId": "b3cae0a7-5fd4-4cb6-f899-7110e4ca413e"
      },
      "source": [
        "#-----LOOP TO FIND THE BEST PARAMETER SETS\n",
        "#INITIATE VALUES TO LOOP THROUGH\n",
        "stop_word_array = np.arange(10,30,1)\n",
        "coef_array = np.logspace(-5,0,30, endpoint=True)\n",
        "\n",
        "#INITIATE VARIABLES TO STORE BEST PARAMETERS\n",
        "coef_best = 0\n",
        "stop_word_best = 0\n",
        "accuracy = 0\n",
        "\n",
        "for stop_word in stop_word_array:\n",
        "    for coef in coef_array:\n",
        "        accuracy_new = k_fold_cross_validation('corpus2_train.labels',5,stop_word,'dir', coef)\n",
        "        if accuracy_new > accuracy:\n",
        "            accuracy = accuracy_new\n",
        "            coef_best = coef\n",
        "            stop_word_best = stop_word\n",
        "print(\"The best parameters for Dirichlet smoothing are stop_word:%d  coef:%1.5f\" %(stop_word_best,coef_best))"
      ],
      "id": "sensitive-surrey",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters for Dirichlet smoothing are stop_word:22  coef:0.02807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "organic-country",
        "outputId": "2a7b1e6d-3df0-4d0b-df69-65ee8f8bea14"
      },
      "source": [
        "#PRINT ACCURACY WITH THE ABOVE PARAMETERS FOR THREE CORPUS\n",
        "accuracy_corpus1 = k_fold_cross_validation('corpus1_train.labels',10,22,'dir', 0.02807)\n",
        "accuracy_corpus2 = k_fold_cross_validation('corpus2_train.labels',10,22,'dir', 0.02807)\n",
        "accuracy_corpus3 = k_fold_cross_validation('corpus3_train.labels',10,22,'dir', 0.02807)\n",
        "print(\"Accuracy for corpus 1, 2, 3 are %1.5f%%, %1.5f%%, %1.5f%% respectively\" %(accuracy_corpus1*100, accuracy_corpus2*100, accuracy_corpus3*100))"
      ],
      "id": "organic-country",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for corpus 1, 2, 3 are 84.06780%, 81.43177%, 86.07330% respectively\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "revised-clark",
        "outputId": "e18f77c1-c58d-4b8b-c91f-9e8abb9a406e"
      },
      "source": [
        "#-----LOOP TO FIND THE BEST PARAMETER SETS\n",
        "#INITIATE VALUES TO LOOP THROUGH\n",
        "stop_word_array = np.arange(24,26,1)\n",
        "coef_array = np.arange(0.09,0.11,0.002)\n",
        "\n",
        "#INITIATE VARIABLES TO STORE BEST PARAMETERS\n",
        "coef_best = 0\n",
        "stop_word_best = 0\n",
        "accuracy = 0\n",
        "\n",
        "for stop_word in stop_word_array:\n",
        "    for coef in coef_array:\n",
        "        accuracy_new = k_fold_cross_validation('corpus2_train.labels',5,stop_word,'lap', coef)\n",
        "        if accuracy_new > accuracy:\n",
        "            accuracy = accuracy_new\n",
        "            coef_best = coef\n",
        "            stop_word_best = stop_word\n",
        "print(\"The best parameters for laplace smoothing are stop_word:%d  coef:%1.5f\" %(stop_word_best,coef_best))"
      ],
      "id": "revised-clark",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters for laplace smoothing are stop_word:25  coef:0.10400\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}